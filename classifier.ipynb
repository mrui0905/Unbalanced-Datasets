{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from torcheval.metrics.functional import binary_auprc\n",
    "\n",
    "from util_final import (\n",
    "    get_mlp,\n",
    "    normalize_features,\n",
    "    get_binary_cross_entropy,\n",
    "    get_binary_accuracy,\n",
    "    pbt,\n",
    "    AdamW\n",
    ")\n",
    "\n",
    "from sampling_methods import (\n",
    "    random_undersample,\n",
    "    smote,\n",
    "    knn_undersampling,\n",
    "    tomek_links\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'mps'\n",
    "config = {\n",
    "        \"alpha\": 0.25,\n",
    "        \"dataset_path\": \"creditcard.pt\",\n",
    "        \"device\": device,\n",
    "        \"ensemble_shape\": (64,),\n",
    "        \"features_dtype\": torch.float32,\n",
    "        \"gamma\": 2.0,\n",
    "        \"labels_dtype\": torch.float32,\n",
    "        \"float_dtype\": torch.float32,\n",
    "        \"hyperparameter_raw_init_distributions\": {\n",
    "            \"epsilon\": torch.distributions.Uniform(\n",
    "                torch.tensor(-10, device=device, dtype=torch.float32),\n",
    "                torch.tensor(-5, device=device, dtype=torch.float32)\n",
    "            ),\n",
    "            \"first_moment_decay\": torch.distributions.Uniform(\n",
    "                torch.tensor(-3, device=device, dtype=torch.float32),\n",
    "                torch.tensor(0, device=device, dtype=torch.float32)\n",
    "            ),\n",
    "            \"learning_rate\": torch.distributions.Uniform(\n",
    "                torch.tensor(-5, device=device, dtype=torch.float32),\n",
    "                torch.tensor(-1, device=device, dtype=torch.float32)\n",
    "            ),\n",
    "            \"second_moment_decay\": torch.distributions.Uniform(\n",
    "                torch.tensor(-5, device=device, dtype=torch.float32),\n",
    "                torch.tensor(-1, device=device, dtype=torch.float32)\n",
    "            ),\n",
    "            \"weight_decay\": torch.distributions.Uniform(\n",
    "                torch.tensor(-5, device=device, dtype=torch.float32),\n",
    "                torch.tensor(-1, device=device, dtype=torch.float32)\n",
    "            )\n",
    "        },\n",
    "        \"hyperparameter_raw_perturb\": {\n",
    "            \"epsilon\": torch.distributions.Normal(\n",
    "                torch.tensor(0, device=device, dtype=torch.float32),\n",
    "                torch.tensor(1, device=device, dtype=torch.float32)\n",
    "            ),\n",
    "            \"first_moment_decay\": torch.distributions.Normal(\n",
    "                torch.tensor(0, device=device, dtype=torch.float32),\n",
    "                torch.tensor(1, device=device, dtype=torch.float32)\n",
    "            ),\n",
    "            \"learning_rate\": torch.distributions.Normal(\n",
    "                torch.tensor(0, device=device, dtype=torch.float32),\n",
    "                torch.tensor(1, device=device, dtype=torch.float32)\n",
    "            ),\n",
    "            \"second_moment_decay\": torch.distributions.Normal(\n",
    "                torch.tensor(0, device=device, dtype=torch.float32),\n",
    "                torch.tensor(1, device=device, dtype=torch.float32)\n",
    "            ),\n",
    "            \"weight_decay\": torch.distributions.Normal(\n",
    "                torch.tensor(0, device=device, dtype=torch.float32),\n",
    "                torch.tensor(1, device=device, dtype=torch.float32)\n",
    "            )\n",
    "        },\n",
    "        \"hyperparameter_transforms\": {\n",
    "            \"epsilon\": lambda log10: 10 ** log10,\n",
    "            \"first_moment_decay\": lambda x: (1 - 10 ** x).clamp(0, 1),\n",
    "            \"learning_rate\": lambda log10: 10 ** log10,\n",
    "            \"second_moment_decay\": lambda x: (1 - 10 ** x).clamp(0, 1),\n",
    "            \"weight_decay\": lambda log10: 10 ** log10\n",
    "        },\n",
    "        \"improvement_threshold\": 1e-4,\n",
    "        \"minibatch_size\": 128,\n",
    "        \"minibatch_size_eval\": 1 << 8,\n",
    "        \"pbt\": True,\n",
    "        \"seed\": 0,\n",
    "        \"steps_num\": 100_000,\n",
    "        \"steps_without_improvement\": 1000,\n",
    "        \"valid_interval\": 1000,\n",
    "        \"welch_confidence_level\": .95,\n",
    "        \"welch_sample_size\": 10,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binary_auprc(\n",
    "    config: dict,\n",
    "    logits: torch.Tensor,\n",
    "    labels: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Get the binary accuracy between a label and a logit tensor.\n",
    "    It can handle arbitrary ensemble shapes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    logits : torch.Tensor\n",
    "        The logit tensor. We assume it has shape\n",
    "        `ensemble_shape + (dataset_size, 1)`.\n",
    "    labels : torch.Tensor\n",
    "        The tensor of true labels. We assume it has shape\n",
    "        `(dataset_size,)` or `ensemble_shape + (dataset_size,)`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The tensor of binary accuracies per ensemble member\n",
    "    of shape `ensemble_shape`.\n",
    "    \"\"\"\n",
    "    logit_positive = logits[..., 0]\n",
    "    prob_positive = torch.sigmoid(logit_positive)\n",
    "    true_positives = labels.broadcast_to(\n",
    "        prob_positive.shape\n",
    "    ).to(torch.bool)\n",
    "    if len(logits.shape) == 1:\n",
    "        num_tasks = 1\n",
    "    else:\n",
    "        num_tasks = logits.shape[0]\n",
    "    return binary_auprc(prob_positive, true_positives, num_tasks=num_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(config[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    dataset = torch.load(config['dataset_path'], weights_only=True)\n",
    "    train_features, train_labels = dataset['train_features'], dataset['train_labels']\n",
    "    test_features, test_labels = dataset['test_features'], dataset['test_labels']\n",
    "\n",
    "    train_features = train_features.to(dtype=config['features_dtype'], device=config['device'])\n",
    "    train_labels = train_labels.to(dtype=config['labels_dtype'], device=config['device'])\n",
    "    test_features = test_features.to(dtype=config['features_dtype'], device=config['device'])\n",
    "    test_labels = test_labels.to(dtype=config['labels_dtype'], device=config['device'])\n",
    "\n",
    "    return train_features, train_labels, test_features, test_labels\n",
    "\n",
    "def train_valid_split(train_features, train_labels, test_size=0.1):\n",
    "    train_features_np = train_features.detach().cpu().numpy()\n",
    "    train_labels_np = train_labels.detach().cpu().numpy()\n",
    "\n",
    "    train_features, valid_features, train_labels, valid_labels = train_test_split(\n",
    "        train_features_np, train_labels_np, test_size=test_size, stratify=train_labels_np, random_state=config['seed']\n",
    "    )\n",
    "\n",
    "    train_features = torch.tensor(train_features, device=config['device'], dtype=config['features_dtype'])\n",
    "    valid_features = torch.tensor(valid_features, device=config['device'], dtype=config['features_dtype'])\n",
    "    train_labels = torch.tensor(train_labels, device=config['device'], dtype=config['labels_dtype'])\n",
    "    valid_labels = torch.tensor(valid_labels, device=config['device'], dtype=config['labels_dtype'])\n",
    "\n",
    "    return train_features, train_labels, valid_features, valid_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(hyperparameters, sampling_procedure, loss_func=get_binary_cross_entropy):\n",
    "    log = {}\n",
    "\n",
    "    for param in hyperparameters:\n",
    "        train_features, train_labels, valid_features, valid_labels, test_features, test_labels = sampling_procedure(param)\n",
    "\n",
    "        model = get_mlp(config, train_features.shape[-1], 1, 3, 128)\n",
    "        optimizer = AdamW(model.parameters())\n",
    "\n",
    "        output = pbt(\n",
    "            config,\n",
    "            loss_func,\n",
    "            get_binary_accuracy,\n",
    "            model,\n",
    "            optimizer,\n",
    "            train_features,\n",
    "            train_labels,\n",
    "            valid_features,\n",
    "            valid_labels\n",
    "        )\n",
    "\n",
    "        pred_logits = model(test_features)\n",
    "        auprc = get_binary_auprc(\n",
    "            config,\n",
    "            pred_logits,\n",
    "            test_labels\n",
    "        )\n",
    "        \n",
    "        accuracy = get_binary_accuracy(\n",
    "            config,\n",
    "            pred_logits,\n",
    "            test_labels\n",
    "        )\n",
    "\n",
    "        log[str(param)] = {\n",
    "            \"auprc\" : auprc.max().item(),\n",
    "            \"accuracy\" : accuracy.max().item(),\n",
    "            \"output\" : output\n",
    "        }\n",
    "\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline(param):\n",
    "    train_features, train_labels, test_features, test_labels = load_data()\n",
    "    train_features, train_labels, valid_features, valid_labels = train_valid_split(train_features, train_labels, test_size=test_features.shape[0])\n",
    "\n",
    "    normalize_features(\n",
    "        train_features,\n",
    "        (valid_features, test_features),\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    return train_features, train_labels, valid_features, valid_labels, test_features, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersample_random(size):\n",
    "    train_features, train_labels, test_features, test_labels = load_data()\n",
    "\n",
    "    positive_mask = train_labels > 0\n",
    "    positive_features = train_features[positive_mask]\n",
    "    negative_features = train_features[~positive_mask]\n",
    "    positive_labels = train_labels[positive_mask]\n",
    "    negative_labels = train_labels[~positive_mask]\n",
    "\n",
    "    negative_features, negative_labels = random_undersample(negative_features, negative_labels, size)\n",
    "\n",
    "    train_features = torch.cat((positive_features, negative_features), dim=0)\n",
    "    train_labels = torch.cat((positive_labels, negative_labels), dim=0)\n",
    "    indices = torch.randperm(train_features.shape[0])\n",
    "    train_features = train_features[indices]\n",
    "    train_labels = train_labels[indices]\n",
    "\n",
    "    train_features, train_labels, valid_features, valid_labels = train_valid_split(train_features, train_labels)\n",
    "\n",
    "    normalize_features(\n",
    "        train_features,\n",
    "        (valid_features, test_features),\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    return train_features, train_labels, valid_features, valid_labels, test_features, test_labels\n",
    "\n",
    "def undersample_tomek(param):\n",
    "    train_features, train_labels, test_features, test_labels = load_data()\n",
    "    train_features, train_labels = tomek_links(config, train_features, train_labels, 1)\n",
    "    train_features, train_labels, valid_features, valid_labels = train_valid_split(train_features, train_labels)\n",
    "\n",
    "    normalize_features(\n",
    "        train_features,\n",
    "        (valid_features, test_features),\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    return train_features, train_labels, valid_features, valid_labels, test_features, test_labels\n",
    "\n",
    "def undersample_knn(k):\n",
    "    train_features, train_labels, test_features, test_labels = load_data()\n",
    "    train_features, train_labels = knn_undersampling(config, train_features, train_labels, 1, k)\n",
    "    train_features, train_labels, valid_features, valid_labels = train_valid_split(train_features, train_labels)\n",
    "\n",
    "    normalize_features(\n",
    "        train_features,\n",
    "        (valid_features, test_features),\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    return train_features, train_labels, valid_features, valid_labels, test_features, test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample_smote(N):\n",
    "    train_features, train_labels, test_features, test_labels = load_data()\n",
    "\n",
    "    positive_mask = train_labels > 0\n",
    "    positive_features = train_features[positive_mask]\n",
    "    negative_features = train_features[~positive_mask]\n",
    "    positive_labels = train_labels[positive_mask]\n",
    "    negative_labels = train_labels[~positive_mask]\n",
    "\n",
    "    positive_features, positive_labels = smote(config, positive_features, positive_labels, 1, N)\n",
    "\n",
    "    train_features = torch.cat((positive_features, negative_features), dim=0)\n",
    "    train_labels = torch.cat((positive_labels, negative_labels), dim=0)\n",
    "    indices = torch.randperm(train_features.shape[0])\n",
    "    train_features = train_features[indices]\n",
    "    train_labels = train_labels[indices]\n",
    "\n",
    "    train_features, train_labels, valid_features, valid_labels = train_valid_split(train_features, train_labels)\n",
    "\n",
    "    normalize_features(\n",
    "        train_features,\n",
    "        (valid_features, test_features),\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    return train_features, train_labels, valid_features, valid_labels, test_features, test_labels\n",
    "\n",
    "def smote_random_undersample(params):\n",
    "    N, size = params\n",
    "    train_features, train_labels, test_features, test_labels = load_data()\n",
    "\n",
    "    positive_mask = train_labels > 0\n",
    "    positive_features = train_features[positive_mask]\n",
    "    negative_features = train_features[~positive_mask]\n",
    "    positive_labels = train_labels[positive_mask]\n",
    "    negative_labels = train_labels[~positive_mask]\n",
    "\n",
    "    positive_features, positive_labels = smote(config, positive_features, positive_labels, 1, N)\n",
    "    negative_features, negative_labels = random_undersample(negative_features, negative_labels, size)\n",
    "\n",
    "    train_features = torch.cat((positive_features, negative_features), dim=0)\n",
    "    train_labels = torch.cat((positive_labels, negative_labels), dim=0)\n",
    "    indices = torch.randperm(train_features.shape[0])\n",
    "    train_features = train_features[indices]\n",
    "    train_labels = train_labels[indices]\n",
    "\n",
    "    train_features, train_labels, valid_features, valid_labels = train_valid_split(train_features, train_labels)\n",
    "\n",
    "    normalize_features(\n",
    "        train_features,\n",
    "        (valid_features, test_features),\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    return train_features, train_labels, valid_features, valid_labels, test_features, test_labels\n",
    "\n",
    "def smote_tomek(N):\n",
    "    train_features, train_labels, test_features, test_labels = load_data()\n",
    "\n",
    "    positive_mask = train_labels > 0\n",
    "    positive_features = train_features[positive_mask]\n",
    "    negative_features = train_features[~positive_mask]\n",
    "    positive_labels = train_labels[positive_mask]\n",
    "    negative_labels = train_labels[~positive_mask]\n",
    "\n",
    "    positive_features, positive_labels = smote(config, positive_features, positive_labels, 1, N)\n",
    "\n",
    "    train_features = torch.cat((positive_features, negative_features), dim=0)\n",
    "    train_labels = torch.cat((positive_labels, negative_labels), dim=0)\n",
    "    indices = torch.randperm(train_features.shape[0])\n",
    "    train_features = train_features[indices]\n",
    "    train_labels = train_labels[indices]\n",
    "\n",
    "    train_features, train_labels = tomek_links(config, train_features, train_labels, 1)\n",
    "\n",
    "    train_features, train_labels, valid_features, valid_labels = train_valid_split(train_features, train_labels)\n",
    "\n",
    "    normalize_features(\n",
    "        train_features,\n",
    "        (valid_features, test_features),\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    return train_features, train_labels, valid_features, valid_labels, test_features, test_labels\n",
    "\n",
    "def smote_knn(params):\n",
    "    N, k = params\n",
    "    train_features, train_labels, test_features, test_labels = load_data()\n",
    "\n",
    "    positive_mask = train_labels > 0\n",
    "    positive_features = train_features[positive_mask]\n",
    "    negative_features = train_features[~positive_mask]\n",
    "    positive_labels = train_labels[positive_mask]\n",
    "    negative_labels = train_labels[~positive_mask]\n",
    "\n",
    "    positive_features, positive_labels = smote(config, positive_features, positive_labels, 1, N)\n",
    "\n",
    "\n",
    "    train_features = torch.cat((positive_features, negative_features), dim=0)\n",
    "    train_labels = torch.cat((positive_labels, negative_labels), dim=0)\n",
    "    indices = torch.randperm(train_features.shape[0])\n",
    "    train_features = train_features[indices]\n",
    "    train_labels = train_labels[indices]\n",
    "\n",
    "    train_features, train_labels = knn_undersampling(config, train_features, train_labels, 1, k)\n",
    "    train_features, train_labels, valid_features, valid_labels = train_valid_split(train_features, train_labels)\n",
    "\n",
    "    normalize_features(\n",
    "        train_features,\n",
    "        (valid_features, test_features),\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    return train_features, train_labels, valid_features, valid_labels, test_features, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binary_focal_loss(\n",
    "    config: dict,\n",
    "    logits: torch.Tensor,\n",
    "    labels: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the focal loss between a label and a logit tensor.\n",
    "    It can handle arbitrary ensemble shapes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config: dict\n",
    "        Expects keys 'alpha' and 'gamma' with float values\n",
    "    logits : torch.Tensor\n",
    "        The logit tensor. We assume it has shape\n",
    "        `ensemble_shape + (dataset_size,)`.\n",
    "    labels : torch.Tensor\n",
    "        The tensor of true labels. We assume it has shape\n",
    "        `(dataset_size,)` or `ensemble_shape + (dataset_size, 1)`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        The tensor of focal losses per ensemble member\n",
    "        of shape `ensemble_shape`.\n",
    "    \"\"\"\n",
    "    alpha, gamma = config[\"alpha\"], config[\"gamma\"]\n",
    "    \n",
    "    logits = logits[..., 0]\n",
    "    labels = labels.broadcast_to(logits.shape)\n",
    "    \n",
    "    bce_loss = F.binary_cross_entropy_with_logits(\n",
    "        logits,\n",
    "        labels,\n",
    "        reduction='none'\n",
    "    )\n",
    "\n",
    "    probs = torch.sigmoid(logits)\n",
    "    probs = torch.clip(probs, 1e-7, 1-1e-7)\n",
    "\n",
    "    p_t = torch.where(labels == 1, probs, 1 - probs)\n",
    "    modulating_factor = (1 - p_t) ** gamma\n",
    "    alpha_factor = torch.where(labels == 1, alpha, 1 - alpha)\n",
    "    focal_loss = alpha_factor * modulating_factor * bce_loss\n",
    "\n",
    "    return focal_loss.mean(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = grid_search(['0'], baseline, get_binary_focal_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels, test_features, test_labels = load_data()\n",
    "\n",
    "output = grid_search([int(n * train_labels.sum().item()) for n in range(1, 9)], undersample_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = grid_search(['0'], undersample_tomek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = grid_search([k for k in range(50, 250, 50)], undersample_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = grid_search([n for n in range(2, 11)], oversample_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels, test_features, test_labels = load_data()\n",
    "\n",
    "params = [(N, int(size * train_labels.sum().item() * N)) for N in range(9, 11) for size in range(5, 9)]\n",
    "output = grid_search(params, smote_random_undersample)\n",
    "for key in output:\n",
    "    print(key, output[key]['auprc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in output:\n",
    "    print(key, output[key]['auprc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = grid_search([n for n in range(2, 11)], smote_tomek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [(N, k) for N in range(9, 11) for k in range(50, 250, 50)]\n",
    "output = grid_search(params, smote_knn)\n",
    "for key in output:\n",
    "    print(output[key]['auprc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = grid_search(['0'], baseline, get_binary_focal_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
